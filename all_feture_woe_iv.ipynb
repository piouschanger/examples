{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_cal(df):\n",
    "    \"\"\"\n",
    "    df :数据集\n",
    "\n",
    "    return：每个变量的缺失率\n",
    "    \"\"\"\n",
    "    missing_series = df.isnull().sum() / df.shape[0]\n",
    "    missing_df = pd.DataFrame(missing_series).reset_index()\n",
    "    missing_df = missing_df.rename(columns={'index': 'col',\n",
    "                                            0: 'missing_pct'})\n",
    "    missing_df = missing_df.sort_values('missing_pct', ascending=False).reset_index(drop=True)\n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_delete_var(df, threshold=None):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    threshold:缺失率删除的阈值\n",
    "\n",
    "    return :删除缺失后的数据集\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    missing_df = missing_cal(df)\n",
    "    missing_col_num = missing_df[missing_df.missing_pct >= threshold].shape[0]\n",
    "    missing_col = list(missing_df[missing_df.missing_pct >= threshold].col)\n",
    "    df2 = df2.drop(missing_col, axis=1)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_delete_user(df, threshold=None):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    threshold:缺失个数删除的阈值\n",
    "\n",
    "    return :删除缺失后的数据集\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    missing_series = df.isnull().sum(axis=1)\n",
    "    missing_list = list(missing_series)\n",
    "    missing_index_list = []\n",
    "    for i, j in enumerate(missing_list):\n",
    "        if j >= threshold:\n",
    "            missing_index_list.append(i)\n",
    "    df2 = df2[~(df2.index.isin(missing_index_list))]\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_delete(df, col_list, threshold=None):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    col_list:变量list集合\n",
    "    threshold:同值化处理的阈值\n",
    "\n",
    "    return :处理后的数据集\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    const_col = []\n",
    "    for col in col_list:\n",
    "        const_pct = df2[col].value_counts().iloc[0] / df2[df2[col].notnull()].shape[0]\n",
    "        if const_pct >= threshold:\n",
    "            const_col.append(col)\n",
    "    df2 = df2.drop(const_col, axis=1)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(df, target):\n",
    "    \"\"\"\n",
    "    df:包含了label和特征的宽表\n",
    "\n",
    "    return:\n",
    "    df :清洗后的数据集\n",
    "    \"\"\"\n",
    "    # 特征缺失处理\n",
    "    df = missing_delete_var(df, threshold=0.8)\n",
    "    # 样本缺失处理\n",
    "    df = missing_delete_user(df, threshold=int(df.shape[1] * 0.8))\n",
    "    col_list = [x for x in df.columns if x != target]\n",
    "    # 常变量处理\n",
    "    df = const_delete(df, col_list, threshold=0.9)\n",
    "    desc = df.describe().T\n",
    "    # 剔除方差为0的特征\n",
    "    std_0_col = list(desc[desc['std'] == 0].index)\n",
    "    if len(std_0_col) > 0:\n",
    "        df = df.drop(std_0_col, axis=1)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 缺失值计算和填充\n",
    "    miss_df = missing_cal(df)\n",
    "    cate_col = list(df.select_dtypes(include=['O']).columns)\n",
    "    num_col = [x for x in list(df.select_dtypes(include=['int64', 'float64']).columns) if x != 'label']\n",
    "\n",
    "    # 分类型特征填充\n",
    "    cate_miss_col1 = [x for x in list(miss_df[miss_df.missing_pct > 0.05]['col']) if x in cate_col]\n",
    "    cate_miss_col2 = [x for x in list(miss_df[miss_df.missing_pct <= 0.05]['col']) if x in cate_col]\n",
    "    # 连续型特征填充\n",
    "    num_miss_col1 = [x for x in list(miss_df[miss_df.missing_pct > 0.05]['col']) if x in num_col]\n",
    "    num_miss_col2 = [x for x in list(miss_df[miss_df.missing_pct <= 0.05]['col']) if x in num_col]\n",
    "    for col in cate_miss_col1:\n",
    "        df[col] = df[col].fillna('未知')\n",
    "    for col in cate_miss_col2:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    for col in num_miss_col1:\n",
    "        df[col] = df[col].fillna(-999)\n",
    "    for col in num_miss_col2:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    return df, miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ks(df, col, target):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    col:输入的特征\n",
    "    target:好坏标记的字段名\n",
    "\n",
    "    return:\n",
    "    ks: KS值\n",
    "    precision:准确率\n",
    "    tpr:召回率\n",
    "    fpr:打扰率\n",
    "    \"\"\"\n",
    "\n",
    "    bad = df[target].sum()\n",
    "    good = df[target].count() - bad\n",
    "    value_list = list(df[col])\n",
    "    label_list = list(df[target])\n",
    "    value_count = df[col].nunique()\n",
    "\n",
    "    items = sorted(zip(value_list, label_list), key=lambda x: x[0])\n",
    "\n",
    "    value_bin = []\n",
    "    ks_list = []\n",
    "    if value_count <= 200:\n",
    "        for i in sorted(set(value_list)):\n",
    "            value_bin.append(i)\n",
    "            label_bin = [x[1] for x in items if x[0] < i]\n",
    "            badrate = sum(label_bin) / bad\n",
    "            goodrate = (len(label_bin) - sum(label_bin)) / good\n",
    "            ks = abs(goodrate - badrate)\n",
    "            ks_list.append(ks)\n",
    "    else:\n",
    "        for i in range(1, 201):\n",
    "            step = (max(value_list) - min(value_list)) / 200\n",
    "            idx = min(value_list) + i * step\n",
    "            value_bin.append(idx)\n",
    "            label_bin = [x[1] for x in items if x[0] < idx]\n",
    "            badrate = sum(label_bin) / bad\n",
    "            goodrate = (len(label_bin) - sum(label_bin)) / good\n",
    "            ks = abs(goodrate - badrate)\n",
    "            ks_list.append(ks)\n",
    "    ks = round(max(ks_list), 3)\n",
    "\n",
    "    ks_value = [value_bin[i] for i, j in enumerate(ks_list) if j == max(ks_list)][0]\n",
    "    precision = df[(df[col] <= ks_value) & (df[target] == 1)].shape[0] / df[df[col] <= ks_value].shape[0]\n",
    "    tpr = df[(df[col] <= ks_value) & (df[target] == 1)].shape[0] / bad\n",
    "    fpr = df[(df[col] <= ks_value) & (df[target] == 0)].shape[0] / good\n",
    "\n",
    "    return ks, precision, tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先用卡方分箱输出变量的分割点\n",
    "def split_data(df, col, split_num):\n",
    "    \"\"\"\n",
    "    df: 原始数据集\n",
    "    col:需要分箱的变量\n",
    "    split_num:分割点的数量\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    count = df2.shape[0]  # 总样本数\n",
    "    n = math.floor(count / split_num)  # 按照分割点数目等分后每组的样本数\n",
    "    split_index = [i * n for i in range(1, split_num)]  # 分割点的索引\n",
    "    values = sorted(list(df2[col]))  # 对变量的值从小到大进行排序\n",
    "    split_value = [values[i] for i in split_index]  # 分割点对应的value\n",
    "    split_value = sorted(list(set(split_value)))  # 分割点的value去重排序\n",
    "    return split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_group(x, split_bin):\n",
    "    \"\"\"\n",
    "    x:变量的value\n",
    "    split_bin:split_data得出的分割点list\n",
    "    \"\"\"\n",
    "    n = len(split_bin)\n",
    "    if x <= min(split_bin):\n",
    "        return min(split_bin)  # 如果x小于分割点的最小值，则x映射为分割点的最小值\n",
    "    elif x > max(split_bin):  # 如果x大于分割点的最大值，则x映射为分割点的最大值\n",
    "        return 10e10\n",
    "    else:\n",
    "        for i in range(n - 1):\n",
    "            if split_bin[i] < x <= split_bin[i + 1]:  # 如果x在两个分割点之间，则x映射为分割点较大的值\n",
    "                return split_bin[i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_bad_rate(df, col, target, grantRateIndicator=0):\n",
    "    \"\"\"\n",
    "    df:原始数据集\n",
    "    col:原始变量/变量映射后的字段\n",
    "    target:目标变量的字段\n",
    "    grantRateIndicator:是否输出总体的违约率\n",
    "    \"\"\"\n",
    "    total = df.groupby([col])[target].count()\n",
    "    bad = df.groupby([col])[target].sum()\n",
    "    total_df = pd.DataFrame({'total': total})\n",
    "    bad_df = pd.DataFrame({'bad': bad})\n",
    "    regroup = pd.merge(total_df, bad_df, left_index=True, right_index=True, how='left')\n",
    "    regroup = regroup.reset_index()\n",
    "    regroup['bad_rate'] = regroup['bad'] / regroup['total']  # 计算根据col分组后每组的违约率\n",
    "    dict_bad = dict(zip(regroup[col], regroup['bad_rate']))  # 转为字典形式\n",
    "    if grantRateIndicator == 0:\n",
    "        return (dict_bad, regroup)\n",
    "    total_all = df.shape[0]\n",
    "    bad_all = df[target].sum()\n",
    "    all_bad_rate = bad_all / total_all  # 计算总体的违约率\n",
    "    return (dict_bad, regroup, all_bad_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_chi2(df, all_bad_rate):\n",
    "    \"\"\"\n",
    "    df:bin_bad_rate得出的regroup\n",
    "    all_bad_rate:bin_bad_rate得出的总体违约率\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2['expected'] = df2['total'] * all_bad_rate  # 计算每组的坏用户期望数量\n",
    "    combined = zip(df2['expected'], df2['bad'])  # 遍历每组的坏用户期望数量和实际数量\n",
    "    chi = [(i[0] - i[1]) ** 2 / i[0] for i in combined]  # 计算每组的卡方值\n",
    "    chi2 = sum(chi)  # 计算总的卡方值\n",
    "    return chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_bin(x, cutoffpoints):\n",
    "    \"\"\"\n",
    "    x:变量的value\n",
    "    cutoffpoints:分箱的切割点\n",
    "    \"\"\"\n",
    "    bin_num = len(cutoffpoints) + 1  # 箱体个数\n",
    "    if x <= cutoffpoints[0]:  # 如果x小于最小的cutoff点，则映射为Bin 0\n",
    "        return 'Bin 0'\n",
    "    elif x > cutoffpoints[-1]:  # 如果x大于最大的cutoff点，则映射为Bin(bin_num-1)\n",
    "        return 'Bin {}'.format(bin_num - 1)\n",
    "    else:\n",
    "        for i in range(0, bin_num - 1):\n",
    "            if cutoffpoints[i] < x <= cutoffpoints[i + 1]:  # 如果x在两个cutoff点之间，则x映射为Bin(i+1)\n",
    "                return 'Bin {}'.format(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChiMerge(df, col, target, max_bin=5, min_binpct=0):\n",
    "    col_unique = sorted(list(set(df[col])))  # 变量的唯一值并排序\n",
    "    n = len(col_unique)  # 变量唯一值得个数\n",
    "    df2 = df.copy()\n",
    "    if n > 100:  # 如果变量的唯一值数目超过100，则将通过split_data和assign_group将x映射为split对应的value\n",
    "        split_col = split_data(df2, col, 100)  # 通过这个目的将变量的唯一值数目人为设定为100\n",
    "        df2['col_map'] = df2[col].map(lambda x: assign_group(x, split_col))\n",
    "    else:\n",
    "        df2['col_map'] = df2[col]  # 变量的唯一值数目没有超过100，则不用做映射\n",
    "    # 生成dict_bad,regroup,all_bad_rate的元组\n",
    "    (dict_bad, regroup, all_bad_rate) = bin_bad_rate(df2, 'col_map', target, grantRateIndicator=1)\n",
    "    col_map_unique = sorted(list(set(df2['col_map'])))  # 对变量映射后的value进行去重排序\n",
    "    group_interval = [[i] for i in col_map_unique]  # 对col_map_unique中每个值创建list并存储在group_interval中\n",
    "\n",
    "    while (len(group_interval) > max_bin):  # 当group_interval的长度大于max_bin时，执行while循环\n",
    "        chi_list = []\n",
    "        for i in range(len(group_interval) - 1):\n",
    "            temp_group = group_interval[i] + group_interval[i + 1]  # temp_group 为生成的区间,list形式，例如[1,3]\n",
    "            chi_df = regroup[regroup['col_map'].isin(temp_group)]\n",
    "            chi_value = cal_chi2(chi_df, all_bad_rate)  # 计算每一对相邻区间的卡方值\n",
    "            chi_list.append(chi_value)\n",
    "        best_combined = chi_list.index(min(chi_list))  # 最小的卡方值的索引\n",
    "        # 将卡方值最小的一对区间进行合并\n",
    "        group_interval[best_combined] = group_interval[best_combined] + group_interval[best_combined + 1]\n",
    "        # 删除合并前的右区间\n",
    "        group_interval.remove(group_interval[best_combined + 1])\n",
    "        # 对合并后每个区间进行排序\n",
    "    group_interval = [sorted(i) for i in group_interval]\n",
    "    # cutoff点为每个区间的最大值\n",
    "    cutoffpoints = [max(i) for i in group_interval[:-1]]\n",
    "\n",
    "    # 检查是否有箱只有好样本或者只有坏样本\n",
    "    df2['col_map_bin'] = df2['col_map'].apply(lambda x: assign_bin(x, cutoffpoints))  # 将col_map映射为对应的区间Bin\n",
    "    # 计算每个区间的违约率\n",
    "    (dict_bad, regroup) = bin_bad_rate(df2, 'col_map_bin', target)\n",
    "    # 计算最小和最大的违约率\n",
    "    [min_bad_rate, max_bad_rate] = [min(dict_bad.values()), max(dict_bad.values())]\n",
    "    # 当最小的违约率等于0，说明区间内只有好样本，当最大的违约率等于1，说明区间内只有坏样本\n",
    "    while min_bad_rate == 0 or max_bad_rate == 1:\n",
    "        bad01_index = regroup[regroup['bad_rate'].isin([0, 1])].col_map_bin.tolist()  # 违约率为1或0的区间\n",
    "        bad01_bin = bad01_index[0]\n",
    "        if bad01_bin == max(regroup.col_map_bin):\n",
    "            cutoffpoints = cutoffpoints[:-1]  # 当bad01_bin是最大的区间时，删除最大的cutoff点\n",
    "        elif bad01_bin == min(regroup.col_map_bin):\n",
    "            cutoffpoints = cutoffpoints[1:]  # 当bad01_bin是最小的区间时，删除最小的cutoff点\n",
    "        else:\n",
    "            bad01_bin_index = list(regroup.col_map_bin).index(bad01_bin)  # 找出bad01_bin的索引\n",
    "            prev_bin = list(regroup.col_map_bin)[bad01_bin_index - 1]  # bad01_bin前一个区间\n",
    "            df3 = df2[df2.col_map_bin.isin([prev_bin, bad01_bin])]\n",
    "            (dict_bad, regroup1) = bin_bad_rate(df3, 'col_map_bin', target)\n",
    "            chi1 = cal_chi2(regroup1, all_bad_rate)  # 计算前一个区间和bad01_bin的卡方值\n",
    "            later_bin = list(regroup.col_map_bin)[bad01_bin_index + 1]  # bin01_bin的后一个区间\n",
    "            df4 = df2[df2.col_map_bin.isin([later_bin, bad01_bin])]\n",
    "            (dict_bad, regroup2) = bin_bad_rate(df4, 'col_map_bin', target)\n",
    "            chi2 = cal_chi2(regroup2, all_bad_rate)  # 计算后一个区间和bad01_bin的卡方值\n",
    "            if chi1 < chi2:  # 当chi1<chi2时,删除前一个区间对应的cutoff点\n",
    "                cutoffpoints.remove(cutoffpoints[bad01_bin_index - 1])\n",
    "            else:  # 当chi1>=chi2时,删除bin01对应的cutoff点\n",
    "                cutoffpoints.remove(cutoffpoints[bad01_bin_index])\n",
    "        df2['col_map_bin'] = df2['col_map'].apply(lambda x: assign_bin(x, cutoffpoints))\n",
    "        (dict_bad, regroup) = bin_bad_rate(df2, 'col_map_bin', target)\n",
    "        # 重新将col_map映射至区间，并计算最小和最大的违约率，直达不再出现违约率为0或1的情况，循环停止\n",
    "        [min_bad_rate, max_bad_rate] = [min(dict_bad.values()), max(dict_bad.values())]\n",
    "\n",
    "    # 检查分箱后的最小占比\n",
    "    if min_binpct > 0:\n",
    "        group_values = df2['col_map'].apply(lambda x: assign_bin(x, cutoffpoints))\n",
    "        df2['col_map_bin'] = group_values  # 将col_map映射为对应的区间Bin\n",
    "        group_df = group_values.value_counts().to_frame()\n",
    "        group_df['bin_pct'] = group_df['col_map'] / n  # 计算每个区间的占比\n",
    "        min_pct = group_df.bin_pct.min()  # 得出最小的区间占比\n",
    "        while min_pct < min_binpct and len(cutoffpoints) > 2:  # 当最小的区间占比小于min_pct且cutoff点的个数大于2，执行循环\n",
    "            # 下面的逻辑基本与“检验是否有箱体只有好/坏样本”的一致\n",
    "            min_pct_index = group_df[group_df.bin_pct == min_pct].index.tolist()\n",
    "            min_pct_bin = min_pct_index[0]\n",
    "            if min_pct_bin == max(group_df.index):\n",
    "                cutoffpoints = cutoffpoints[:-1]\n",
    "            elif min_pct_bin == min(group_df.index):\n",
    "                cutoffpoints = cutoffpoints[1:]\n",
    "            else:\n",
    "                minpct_bin_index = list(group_df.index).index(min_pct_bin)\n",
    "                prev_pct_bin = list(group_df.index)[minpct_bin_index - 1]\n",
    "                df5 = df2[df2['col_map_bin'].isin([min_pct_bin, prev_pct_bin])]\n",
    "                (dict_bad, regroup3) = bin_bad_rate(df5, 'col_map_bin', target)\n",
    "                chi3 = cal_chi2(regroup3, all_bad_rate)\n",
    "                later_pct_bin = list(group_df.index)[minpct_bin_index + 1]\n",
    "                df6 = df2[df2['col_map_bin'].isin([min_pct_bin, later_pct_bin])]\n",
    "                (dict_bad, regroup4) = bin_bad_rate(df6, 'col_map_bin', target)\n",
    "                chi4 = cal_chi2(regroup4, all_bad_rate)\n",
    "                if chi3 < chi4:\n",
    "                    cutoffpoints.remove(cutoffpoints[minpct_bin_index - 1])\n",
    "                else:\n",
    "                    cutoffpoints.remove(cutoffpoints[minpct_bin_index])\n",
    "    return cutoffpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_cate(df, col, target):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    col:输入的特征\n",
    "    target:好坏标记的字段名\n",
    "\n",
    "    return:\n",
    "    bin_df :特征的评估结果\n",
    "    \"\"\"\n",
    "\n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total - bad\n",
    "    d1 = df.groupby([col], as_index=True)\n",
    "    d2 = pd.DataFrame()\n",
    "    d2['样本数'] = d1[target].count()\n",
    "    d2['黑样本数'] = d1[target].sum()\n",
    "    d2['白样本数'] = d2['样本数'] - d2['黑样本数']\n",
    "    d2['逾期用户占比'] = d2['黑样本数'] / d2['样本数']\n",
    "    d2['badattr'] = d2['黑样本数'] / bad\n",
    "    d2['goodattr'] = d2['白样本数'] / good\n",
    "    d2['WOE'] = np.log(d2['badattr'] / d2['goodattr'])\n",
    "    d2['bin_iv'] = (d2['badattr'] - d2['goodattr']) * d2['WOE']\n",
    "    d2['IV'] = d2['bin_iv'].sum()\n",
    "\n",
    "    bin_df = d2.reset_index()\n",
    "    bin_df.drop(['badattr', 'goodattr', 'bin_iv'], axis=1, inplace=True)\n",
    "    bin_df.rename(columns={col: '分箱结果'}, inplace=True)\n",
    "    bin_df['特征名'] = col\n",
    "    bin_df = pd.concat([bin_df['特征名'], bin_df.iloc[:, :-1]], axis=1)\n",
    "    return bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_self(df, col, target, cut=None, right_border=True):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    col:输入的特征\n",
    "    target:好坏标记的字段名\n",
    "    cut:总定义划分区间的list\n",
    "    right_border：设定左开右闭、左闭右开\n",
    "\n",
    "    return:\n",
    "    bin_df :特征的评估结果\n",
    "    \"\"\"\n",
    "\n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total - bad\n",
    "    bucket = pd.cut(df[col], cut, right=right_border)\n",
    "    d1 = df.groupby(bucket)\n",
    "    d2 = pd.DataFrame()\n",
    "    d2['样本数'] = d1[target].count()\n",
    "    d2['黑样本数'] = d1[target].sum()\n",
    "    d2['白样本数'] = d2['样本数'] - d2['黑样本数']\n",
    "    d2['逾期用户占比'] = d2['黑样本数'] / d2['样本数']\n",
    "    d2['badattr'] = d2['黑样本数'] / bad\n",
    "    d2['goodattr'] = d2['白样本数'] / good\n",
    "    d2['WOE'] = np.log(d2['badattr'] / d2['goodattr'])\n",
    "    d2['bin_iv'] = (d2['badattr'] - d2['goodattr']) * d2['WOE']\n",
    "    d2['IV'] = d2['bin_iv'].sum()\n",
    "\n",
    "    bin_df = d2.reset_index()\n",
    "    bin_df.drop(['badattr', 'goodattr', 'bin_iv'], axis=1, inplace=True)\n",
    "    bin_df.rename(columns={col: '分箱结果'}, inplace=True)\n",
    "    bin_df['特征名'] = col\n",
    "    bin_df = pd.concat([bin_df['特征名'], bin_df.iloc[:, :-1]], axis=1)\n",
    "\n",
    "    ks, precision, tpr, fpr = cal_ks(df, col, target)\n",
    "    bin_df['准确率'] = precision\n",
    "    bin_df['召回率'] = tpr\n",
    "    bin_df['打扰率'] = fpr\n",
    "    bin_df['KS'] = ks\n",
    "\n",
    "    return bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_num(df, target, col, max_bin=None, min_binpct=None):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    col:输入的特征\n",
    "    target:好坏标记的字段名\n",
    "    max_bin:最大的分箱个数\n",
    "    min_binpct:区间内样本所占总体的最小比\n",
    "\n",
    "    return:\n",
    "    bin_df :特征的评估结果\n",
    "    \"\"\"\n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total - bad\n",
    "    inf = float('inf')\n",
    "    ninf = float('-inf')\n",
    "\n",
    "    cut = ChiMerge(df, col, target, max_bin=max_bin, min_binpct=min_binpct)\n",
    "    cut.insert(0, ninf)\n",
    "    cut.append(inf)\n",
    "    bucket = pd.cut(df[col], cut)\n",
    "    d1 = df.groupby(bucket)\n",
    "    d2 = pd.DataFrame()\n",
    "    d2['样本数'] = d1[target].count()\n",
    "    d2['黑样本数'] = d1[target].sum()\n",
    "    d2['白样本数'] = d2['样本数'] - d2['黑样本数']\n",
    "    d2['逾期用户占比'] = d2['黑样本数'] / d2['样本数']\n",
    "    d2['badattr'] = d2['黑样本数'] / bad\n",
    "    d2['goodattr'] = d2['白样本数'] / good\n",
    "    d2['WOE'] = np.log(d2['badattr'] / d2['goodattr'])\n",
    "    d2['bin_iv'] = (d2['badattr'] - d2['goodattr']) * d2['WOE']\n",
    "    d2['IV'] = d2['bin_iv'].sum()\n",
    "\n",
    "    bin_df = d2.reset_index()\n",
    "    bin_df.drop(['badattr', 'goodattr', 'bin_iv'], axis=1, inplace=True)\n",
    "    bin_df.rename(columns={col: '分箱结果'}, inplace=True)\n",
    "    bin_df['特征名'] = col\n",
    "    bin_df = pd.concat([bin_df['特征名'], bin_df.iloc[:, :-1]], axis=1)\n",
    "\n",
    "    ks, precision, tpr, fpr = cal_ks(df, col, target)\n",
    "    bin_df['准确率'] = precision\n",
    "    bin_df['召回率'] = tpr\n",
    "    bin_df['打扰率'] = fpr\n",
    "    bin_df['KS'] = ks\n",
    "\n",
    "    return bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_sparse_col(df, target, col, max_bin=None, min_binpct=None, sparse_value=None):\n",
    "    \"\"\"\n",
    "    df:数据集\n",
    "    col:输入的特征\n",
    "    target:好坏标记的字段名\n",
    "    max_bin:最大的分箱个数\n",
    "    min_binpct:区间内样本所占总体的最小比\n",
    "    sparse_value:单独分为一箱的value值\n",
    "\n",
    "    return:\n",
    "    bin_df :特征的评估结果\n",
    "    \"\"\"\n",
    "\n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total - bad\n",
    "\n",
    "    # 对稀疏值0值或者缺失值单独分箱\n",
    "    temp1 = df[df[col] == sparse_value]\n",
    "    temp2 = df[~(df[col] == sparse_value)]\n",
    "\n",
    "    bucket_sparse = pd.cut(temp1[col], [float('-inf'), sparse_value])\n",
    "    group1 = temp1.groupby(bucket_sparse)\n",
    "    bin_df1 = pd.DataFrame()\n",
    "    bin_df1['样本数'] = group1[target].count()\n",
    "    bin_df1['黑样本数'] = group1[target].sum()\n",
    "    bin_df1['白样本数'] = bin_df1['样本数'] - bin_df1['黑样本数']\n",
    "    bin_df1['逾期用户占比'] = bin_df1['黑样本数'] / bin_df1['样本数']\n",
    "    bin_df1['badattr'] = bin_df1['黑样本数'] / bad\n",
    "    bin_df1['goodattr'] = bin_df1['白样本数'] / good\n",
    "    bin_df1['WOE'] = np.log(bin_df1['badattr'] / bin_df1['goodattr'])\n",
    "    bin_df1['bin_iv'] = (bin_df1['badattr'] - bin_df1['goodattr']) * bin_df1['WOE']\n",
    "\n",
    "    bin_df1 = bin_df1.reset_index()\n",
    "\n",
    "    # 对剩余部分做卡方分箱\n",
    "    cut = ChiMerge(temp2, col, target, max_bin=max_bin, min_binpct=min_binpct)\n",
    "    cut.insert(0, sparse_value)\n",
    "    cut.append(float('inf'))\n",
    "\n",
    "    bucket = pd.cut(temp2[col], cut)\n",
    "    group2 = temp2.groupby(bucket)\n",
    "    bin_df2 = pd.DataFrame()\n",
    "    bin_df2['样本数'] = group2[target].count()\n",
    "    bin_df2['黑样本数'] = group2[target].sum()\n",
    "    bin_df2['白样本数'] = bin_df2['样本数'] - bin_df2['黑样本数']\n",
    "    bin_df2['逾期用户占比'] = bin_df2['黑样本数'] / bin_df2['样本数']\n",
    "    bin_df2['badattr'] = bin_df2['黑样本数'] / bad\n",
    "    bin_df2['goodattr'] = bin_df2['白样本数'] / good\n",
    "    bin_df2['WOE'] = np.log(bin_df2['badattr'] / bin_df2['goodattr'])\n",
    "    bin_df2['bin_iv'] = (bin_df2['badattr'] - bin_df2['goodattr']) * bin_df2['WOE']\n",
    "\n",
    "    bin_df2 = bin_df2.reset_index()\n",
    "\n",
    "    # 合并分箱结果\n",
    "    bin_df = pd.concat([bin_df1, bin_df2], axis=0)\n",
    "    bin_df['IV'] = bin_df['bin_iv'].sum().round(3)\n",
    "\n",
    "    bin_df.drop(['badattr', 'goodattr', 'bin_iv'], axis=1, inplace=True)\n",
    "    bin_df.rename(columns={col: '分箱结果'}, inplace=True)\n",
    "    bin_df['特征名'] = col\n",
    "    bin_df = pd.concat([bin_df['特征名'], bin_df.iloc[:, :-1]], axis=1)\n",
    "\n",
    "    ks, precision, tpr, fpr = cal_ks(df, col, target)\n",
    "    bin_df['准确率'] = precision\n",
    "    bin_df['召回率'] = tpr\n",
    "    bin_df['打扰率'] = fpr\n",
    "    bin_df['KS'] = ks\n",
    "\n",
    "    return bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_result(df, target):\n",
    "    \"\"\"\"\n",
    "    df-- 含有特征和标签的宽表\n",
    "    target -- 好坏标签字段名\n",
    "\n",
    "    return:\n",
    "    feature_result -- 每个特征的评估结果\n",
    "    \"\"\"\n",
    "    if target not in df.columns:\n",
    "\n",
    "        print('请将特征文件关联样本好坏标签(字段名label)后再重新运行!')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('数据清洗开始')\n",
    "        df, miss_df = data_processing(df, target)\n",
    "        print('数据清洗完成')\n",
    "\n",
    "        cate_col = list(df.select_dtypes(include=['O']).columns)\n",
    "        num_col = [x for x in list(df.select_dtypes(include=['int64', 'float64']).columns) if x != 'label']\n",
    "\n",
    "        # 类别性变量分箱\n",
    "\n",
    "        bin_cate_list = []\n",
    "        for col in cate_col:\n",
    "            bin_cate = binning_cate(df, col, target)\n",
    "            bin_cate['rank'] = list(range(1, bin_cate.shape[0] + 1, 1))\n",
    "            bin_cate_list.append(bin_cate)\n",
    "\n",
    "        # 数值型特征分箱\n",
    "        num_col1 = [x for x in list(miss_df[miss_df.missing_pct > 0.05]['col']) if x in num_col]\n",
    "        num_col2 = [x for x in list(miss_df[miss_df.missing_pct <= 0.05]['col']) if x in num_col]\n",
    "\n",
    "        print('特征分箱开始')\n",
    "        bin_num_list1 = []\n",
    "        err_col1 = []\n",
    "        for col in tqdm(num_col1):\n",
    "            try:\n",
    "                bin_df1 = binning_sparse_col(df, 'label', col, min_binpct=0.05, max_bin=4, sparse_value=-999)\n",
    "                bin_df1['rank'] = list(range(1, bin_df1.shape[0] + 1, 1))\n",
    "                bin_num_list1.append(bin_df1)\n",
    "            except (IndexError,ZeroDivisionError):\n",
    "                err_col1.append(col)\n",
    "            continue\n",
    "\n",
    "        bin_num_list2 = []\n",
    "        err_col2 = []\n",
    "        for col in tqdm(num_col2):\n",
    "            try:\n",
    "                bin_df2 = binning_num(df, 'label', col, min_binpct=0.05, max_bin=5)\n",
    "                bin_df2['rank'] = list(range(1, bin_df2.shape[0] + 1, 1))\n",
    "                bin_num_list2.append(bin_df2)\n",
    "            except (IndexError,ZeroDivisionError):\n",
    "                err_col2.append(col)\n",
    "            continue\n",
    "\n",
    "        # 卡方分箱报错的特征分箱\n",
    "        err_col = err_col1 + err_col2\n",
    "        bin_num_list3 = []\n",
    "        if len(err_col) > 0:\n",
    "            for col in tqdm(err_col):\n",
    "                ninf = float('-inf')\n",
    "                inf = float('inf')\n",
    "                q_25 = df[col].quantile(0.25)\n",
    "                q_50 = df[col].quantile(0.5)\n",
    "                q_75 = df[col].quantile(0.75)\n",
    "\n",
    "                cut = list(sorted(set([ninf, q_25, q_50, q_75, inf])))\n",
    "\n",
    "                bin_df3 = binning_self(df, col, target, cut=cut, right_border=True)\n",
    "                bin_df3['rank'] = list(range(1, bin_df3.shape[0] + 1, 1))\n",
    "                bin_num_list3.append(bin_df3)\n",
    "        print('特征分箱结束')\n",
    "\n",
    "        bin_all_list = bin_num_list1 + bin_num_list2 + bin_num_list3 + bin_cate_list\n",
    "\n",
    "        feature_result = pd.concat(bin_all_list, axis=0)\n",
    "        feature_result = feature_result.sort_values(['IV', 'rank'], ascending=[False, True])\n",
    "        feature_result = feature_result.drop(['rank'], axis=1)\n",
    "        order_col = ['特征名', '分箱结果', '样本数', '黑样本数', '白样本数', '逾期用户占比', 'WOE', 'IV', '准确率', '召回率', '打扰率', 'KS']\n",
    "        feature_result = feature_result[order_col]\n",
    "        return feature_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (412) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据清洗开始\n",
      "数据清洗完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征分箱开始\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.88s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 589/589 [09:27<00:00,  1.27s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 17.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征分箱结束\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:77: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # =============================================================================\n",
    "    #     if len(sys.argv)==1:\n",
    "    #         print('请数据特征数据文件：')\n",
    "    #         sys.exit()\n",
    "    #     feature_file=sys.argv[1]\n",
    "    #     file_path=os.getcwd()\n",
    "    #     if 'xlsx' in feature_file or 'xls' in feature_file:\n",
    "    #         df = pd.read_excel(file_path+'/'+feature_file,encoding='gbk')\n",
    "    #     else:\n",
    "    #         df = pd.read_csv(file_path+'/'+feature_file,encoding='gbk')\n",
    "    #\n",
    "    # =============================================================================\n",
    "    df = pd.read_csv('C:/Users/Wu Ran/Desktop/all_fe.csv')\n",
    "\n",
    "    result_bin = get_feature_result(df, 'label')\n",
    "    result_bin.to_csv('C:/Users/Wu Ran/Desktop/fe_result.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
