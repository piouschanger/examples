{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import math \n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier,_tree\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_psi(df1,df2,col,bin_num=5):\n",
    "    \"\"\"\n",
    "    计算psi\n",
    "    param:\n",
    "        df1 -- 数据集A Dataframe\n",
    "        df2 -- 数据集B Dataframe\n",
    "        col -- 字段名 string\n",
    "        bin_num -- 连续型特征的分箱数 默认为5\n",
    "    return:\n",
    "        psi float\n",
    "        bin_df -- psi明细表 Dataframe\n",
    "    \"\"\"\n",
    "    # 对于离散型特征直接根据类别进行分箱，分箱逻辑以数据集A为准\n",
    "    if df1[col].dtype == np.dtype('object') or df1[col].dtype == np.dtype('bool') or df1[col].nunique()<=bin_num:\n",
    "        bin_df1 = df1[col].value_counts().to_frame().reset_index().\\\n",
    "                        rename(columns={'index':col,col:'total_A'})\n",
    "        bin_df1['totalrate_A'] = bin_df1['total_A']/df1.shape[0]\n",
    "        bin_df2 = df2[col].value_counts().to_frame().reset_index().\\\n",
    "                        rename(columns={'index':col,col:'total_B'})\n",
    "        bin_df2['totalrate_B'] = bin_df2['total_B']/df2.shape[0]\n",
    "    else:\n",
    "        # 这里采用的是等频分箱\n",
    "        bin_series,bin_cut = pd.qcut(df1[col],q=bin_num,duplicates='drop',retbins=True)\n",
    "        bin_cut[0] = float('-inf')\n",
    "        bin_cut[-1] = float('inf')\n",
    "        bucket1 = pd.cut(df1[col],bins=bin_cut)\n",
    "        group1 = df1.groupby(bucket1)\n",
    "        bin_df1=pd.DataFrame()\n",
    "        bin_df1['total_A'] = group1[col].count()\n",
    "        bin_df1['totalrate_A'] = bin_df1['total_A']/df1.shape[0]\n",
    "        bin_df1 = bin_df1.reset_index()\n",
    "\n",
    "        bucket2 = pd.cut(df2[col],bins=bin_cut)\n",
    "        group2 = df2.groupby(bucket2)\n",
    "        bin_df2=pd.DataFrame()\n",
    "        bin_df2['total_B'] = group2[col].count()\n",
    "        bin_df2['totalrate_B'] = bin_df2['total_B']/df2.shape[0]\n",
    "        bin_df2 = bin_df2.reset_index()\n",
    "    # 计算psi\n",
    "    bin_df = pd.merge(bin_df1,bin_df2,on=col)\n",
    "    bin_df['a'] = bin_df['totalrate_B'] - bin_df['totalrate_A']\n",
    "    bin_df['b'] = np.log(bin_df['totalrate_B']/bin_df['totalrate_A'])\n",
    "    bin_df['Index'] = bin_df['a']*bin_df['b']\n",
    "    bin_df['PSI'] = bin_df['Index'].sum()\n",
    "    bin_df = bin_df.drop(['a','b'],axis=1)\n",
    "    \n",
    "    psi =bin_df.PSI.iloc[0]\n",
    "    \n",
    "    return psi,bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_split(df,col,target,max_bin,min_binpct,nan_value):\n",
    "    \"\"\"\n",
    "    决策树分箱\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        col -- 分箱的字段名 string\n",
    "        target -- 标签的字段名 string\n",
    "        max_bin -- 最大分箱数 int\n",
    "        min_binpct -- 箱体的最小占比 float\n",
    "        nan_value -- 缺失的映射值 int/float\n",
    "    return:\n",
    "        split_list -- 分割点 list\n",
    "    \"\"\"\n",
    "    miss_value_rate = df[df[col]==nan_value].shape[0]/df.shape[0]\n",
    "    # 如果缺失占比小于5%，则直接对特征进行分箱\n",
    "    if miss_value_rate<0.05:\n",
    "        x = np.array(df[col]).reshape(-1,1)\n",
    "        y = np.array(df[target])\n",
    "        tree = DecisionTreeClassifier(max_leaf_nodes=max_bin,\n",
    "                                  min_samples_leaf = min_binpct)\n",
    "        tree.fit(x,y)\n",
    "        thresholds = tree.tree_.threshold\n",
    "        thresholds = thresholds[thresholds!=_tree.TREE_UNDEFINED]\n",
    "        split_list = sorted(thresholds.tolist())\n",
    "    # 如果缺失占比大于5%，则把缺失单独分为一箱，剩余部分再进行决策树分箱\n",
    "    else:\n",
    "        max_bin2 = max_bin-1\n",
    "        x = np.array(df[~(df[col]==nan_value)][col]).reshape(-1,1)\n",
    "        y = np.array(df[~(df[col]==nan_value)][target])\n",
    "        tree = DecisionTreeClassifier(max_leaf_nodes=max_bin2,\n",
    "                                  min_samples_leaf = min_binpct)\n",
    "        tree.fit(x,y)\n",
    "        thresholds = tree.tree_.threshold\n",
    "        thresholds = thresholds[thresholds!=_tree.TREE_UNDEFINED]\n",
    "        split_list = sorted(thresholds.tolist())\n",
    "        split_list.insert(0,nan_value)\n",
    "    \n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_split(df,col,target,max_bin,nan_value):\n",
    "    \"\"\"\n",
    "    等频分箱\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        col -- 分箱的字段名 string\n",
    "        target -- 标签的字段名 string\n",
    "        max_bin -- 最大分箱数 int\n",
    "        nan_value -- 缺失的映射值 int/float\n",
    "    return:\n",
    "        split_list -- 分割点 list\n",
    "    \"\"\"\n",
    "    miss_value_rate = df[df[col]==nan_value].shape[0]/df.shape[0]\n",
    "    \n",
    "    # 如果缺失占比小于5%，则直接对特征进行分箱\n",
    "    if miss_value_rate<0.05:\n",
    "        bin_series,bin_cut = pd.qcut(df[col],q=max_bin,duplicates='drop',retbins=True)\n",
    "        split_list = bin_cut.tolist()\n",
    "        split_list.remove(split_list[0])\n",
    "    # 如果缺失占比大于5%，则把缺失单独分为一箱，剩余部分再进行等频分箱\n",
    "    else:\n",
    "        df2 = df[~(df[col]==nan_value)]\n",
    "        max_bin2 = max_bin-1\n",
    "        bin_series,bin_cut = pd.qcut(df2[col],q=max_bin2,duplicates='drop',retbins=True)\n",
    "        split_list = bin_cut.tolist()\n",
    "        split_list[0] = nan_value\n",
    "        \n",
    "    split_list.remove(split_list[-1])\n",
    "    \n",
    "    # 当出现某个箱体只有好用户或只有坏用户时，进行前向合并箱体\n",
    "    var_arr = np.array(df[col])\n",
    "    target_arr = np.array(df[target])\n",
    "    bin_trans = np.digitize(var_arr,split_list,right=True)\n",
    "    var_tuple = [(x,y) for x,y in zip(bin_trans,target_arr)]\n",
    "    \n",
    "    delete_cut_list = []\n",
    "    for i in set(bin_trans):\n",
    "        target_list = [y for x,y in var_tuple if x==i]\n",
    "        if target_list.count(1)==0 or target_list.count(0)==0:\n",
    "            if i ==min(bin_trans):\n",
    "                index=i\n",
    "            else:\n",
    "                index = i-1\n",
    "            delete_cut_list.append(split_list[index])\n",
    "    split_list = [x for x in split_list if x not in delete_cut_list]\n",
    "    \n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_woe(df,col,target,nan_value,cut=None):\n",
    "    \"\"\"\n",
    "    计算woe\n",
    "    param：\n",
    "        df -- 数据集 Dataframe\n",
    "        col -- 分箱的字段名 string\n",
    "        target -- 标签的字段名 string\n",
    "        nan_value -- 缺失的映射值 int/float\n",
    "        cut -- 箱体分割点 list\n",
    "    return:\n",
    "        woe_list -- 每个箱体的woe list\n",
    "    \"\"\"\n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total-bad\n",
    "    \n",
    "    bucket = pd.cut(df[col],cut)\n",
    "    group = df.groupby(bucket)\n",
    "        \n",
    "    bin_df = pd.DataFrame()\n",
    "    bin_df['total'] = group[target].count()\n",
    "    bin_df['bad'] = group[target].sum()\n",
    "    bin_df['good'] = bin_df['total'] - bin_df['bad']\n",
    "    bin_df['badattr'] = bin_df['bad']/bad\n",
    "    bin_df['goodattr'] = bin_df['good']/good\n",
    "    bin_df['woe'] = np.log(bin_df['badattr']/bin_df['goodattr'])\n",
    "    # 当cut里有缺失映射值时，说明是把缺失单独分为一箱的，后续在进行调成单调分箱时\n",
    "    # 不考虑缺失的箱，故将缺失映射值剔除\n",
    "    if nan_value in cut:\n",
    "        woe_list = bin_df['woe'].tolist()[1:]\n",
    "    else:\n",
    "        woe_list = bin_df['woe'].tolist()\n",
    "    return woe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"woe调成单调递减或单调递增\"\"\"\n",
    "def monot_trim(df,col,target,nan_value,cut=None):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        col -- 分箱的字段名 string\n",
    "        target -- 标签的字段名 string\n",
    "        nan_value -- 缺失的映射值 int/float\n",
    "        cut -- 箱体分割点 list\n",
    "    return:\n",
    "        new_cut -- 调整后的分割点 list\n",
    "    \"\"\"\n",
    "    woe_lst = cal_woe(df,col,target,nan_value,cut = cut)\n",
    "    # 若第一个箱体大于0，说明特征整体上服从单调递减\n",
    "    if woe_lst[0]>0:\n",
    "        while not judge_decreasing(woe_lst):\n",
    "            # 找出哪几个箱不服从单调递减的趋势\n",
    "            judge_list = [x>y for x, y in zip(woe_lst, woe_lst[1:])]\n",
    "            # 用前向合并箱体的方式，找出需要剔除的分割点的索引，如果有缺失映射值，则索引+1\n",
    "            if nan_value in cut:\n",
    "                index_list = [i+2 for i,j in enumerate(judge_list) if j==False]\n",
    "            else:\n",
    "                index_list = [i+1 for i,j in enumerate(judge_list) if j==False]\n",
    "            new_cut = [j for i,j in enumerate(cut) if i not in index_list]\n",
    "            woe_lst = cal_woe(df,col,target,nan_value,cut = new_cut)\n",
    "    # 若第一个箱体小于0，说明特征整体上服从单调递增\n",
    "    elif woe_lst[0]<0:\n",
    "        while not judge_increasing(woe_lst):\n",
    "            # 找出哪几个箱不服从单调递增的趋势\n",
    "            judge_list = [x<y for x, y in zip(woe_lst, woe_lst[1:])]\n",
    "            # 用前向合并箱体的方式，找出需要剔除的分割点的索引，如果有缺失映射值，则索引+1\n",
    "            if nan_value in cut:\n",
    "                index_list = [i+2 for i,j in enumerate(judge_list) if j==False]\n",
    "            else:\n",
    "                index_list = [i+1 for i,j in enumerate(judge_list) if j==False]\n",
    "            new_cut = [j for i,j in enumerate(cut) if i not in index_list]\n",
    "            woe_lst = cal_woe(df,col,target,nan_value,cut = new_cut)\n",
    "    \n",
    "    return new_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_increasing(L):\n",
    "    \"\"\"\n",
    "    判断一个list是否单调递增\n",
    "    \"\"\"\n",
    "    return all(x<y for x, y in zip(L, L[1:]))\n",
    "\n",
    "def judge_decreasing(L):\n",
    "    \"\"\"\n",
    "    判断一个list是否单调递减\n",
    "    \"\"\"\n",
    "    return all(x>y for x, y in zip(L, L[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_var(df,col,target,bin_type='dt',max_bin=5,min_binpct=0.05,nan_value=-999):\n",
    "    \"\"\"\n",
    "    特征分箱，计算iv\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        col -- 分箱的字段名 string\n",
    "        target -- 标签的字段名 string\n",
    "        bin_type -- 分箱方式 默认是'dt',还有'quantile'(等频分箱)\n",
    "        max_bin -- 最大分箱数 int\n",
    "        min_binpct -- 箱体的最小占比 float\n",
    "        nan_value -- 缺失映射值 int/float\n",
    "    return:\n",
    "        bin_df -- 特征的分箱明细表 Dataframe\n",
    "        cut -- 分割点 list\n",
    "    \"\"\"\n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total-bad\n",
    "    \n",
    "    # 离散型特征分箱,直接根据类别进行groupby\n",
    "    if df[col].dtype == np.dtype('object') or df[col].dtype == np.dtype('bool') or df[col].nunique()<=max_bin:\n",
    "        group = df.groupby([col],as_index=True)\n",
    "        bin_df = pd.DataFrame()\n",
    "\n",
    "        bin_df['total'] = group[target].count()\n",
    "        bin_df['totalrate'] = bin_df['total']/total\n",
    "        bin_df['bad'] = group[target].sum()\n",
    "        bin_df['badrate'] = bin_df['bad']/bin_df['total']\n",
    "        bin_df['good'] = bin_df['total'] - bin_df['bad']\n",
    "        bin_df['goodrate'] = bin_df['good']/bin_df['total']\n",
    "        bin_df['badattr'] = bin_df['bad']/bad\n",
    "        bin_df['goodattr'] = (bin_df['total']-bin_df['bad'])/good\n",
    "        bin_df['woe'] = np.log(bin_df['badattr']/bin_df['goodattr'])\n",
    "        bin_df['bin_iv'] = (bin_df['badattr']-bin_df['goodattr'])*bin_df['woe']\n",
    "        bin_df['IV'] = bin_df['bin_iv'].sum()\n",
    "        cut = df[col].unique().tolist()\n",
    "    # 连续型特征的分箱\n",
    "    else:\n",
    "        if bin_type=='dt':\n",
    "            cut = tree_split(df,col,target,max_bin=max_bin,min_binpct=min_binpct,nan_value=nan_value)\n",
    "        elif bin_type=='quantile':\n",
    "            cut = quantile_split(df,col,target,max_bin=max_bin,nan_value=nan_value)\n",
    "        cut.insert(0,float('-inf'))\n",
    "        cut.append(float('inf'))\n",
    "        \n",
    "        bucket = pd.cut(df[col],cut)\n",
    "        group = df.groupby(bucket)\n",
    "        bin_df = pd.DataFrame()\n",
    "\n",
    "        bin_df['total'] = group[target].count()\n",
    "        bin_df['totalrate'] = bin_df['total']/total\n",
    "        bin_df['bad'] = group[target].sum()\n",
    "        bin_df['badrate'] = bin_df['bad']/bin_df['total']\n",
    "        bin_df['good'] = bin_df['total'] - bin_df['bad']\n",
    "        bin_df['goodrate'] = bin_df['good']/bin_df['total']\n",
    "        bin_df['badattr'] = bin_df['bad']/bad\n",
    "        bin_df['goodattr'] = (bin_df['total']-bin_df['bad'])/good\n",
    "        bin_df['woe'] = np.log(bin_df['badattr']/bin_df['goodattr'])\n",
    "        bin_df['bin_iv'] = (bin_df['badattr']-bin_df['goodattr'])*bin_df['woe']\n",
    "        bin_df['IV'] = bin_df['bin_iv'].sum()\n",
    "        \n",
    "    return bin_df,cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_trim(df,col,target,cut=None,right_border=True):\n",
    "    \"\"\"\n",
    "    调整单调后的分箱，计算IV\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        col -- 分箱的字段名 string\n",
    "        target -- 标签的字段名 string\n",
    "        cut -- 分割点 list\n",
    "        right_border -- 箱体的右边界是否闭合 bool\n",
    "    return:\n",
    "        bin_df -- 特征的分箱明细表 Dataframe\n",
    "    \"\"\"\n",
    "    total = df[target].count()\n",
    "    bad = df[target].sum()\n",
    "    good = total - bad\n",
    "    bucket = pd.cut(df[col],cut,right=right_border)\n",
    "    \n",
    "    group = df.groupby(bucket)\n",
    "    bin_df = pd.DataFrame()\n",
    "    bin_df['total'] = group[target].count()\n",
    "    bin_df['totalrate'] = bin_df['total']/total\n",
    "    bin_df['bad'] = group[target].sum()\n",
    "    bin_df['badrate'] = bin_df['bad']/bin_df['total']\n",
    "    bin_df['good'] = bin_df['total'] - bin_df['bad']\n",
    "    bin_df['goodrate'] = bin_df['good']/bin_df['total']\n",
    "    bin_df['badattr'] = bin_df['bad']/bad\n",
    "    bin_df['goodattr'] = (bin_df['total']-bin_df['bad'])/good\n",
    "    bin_df['woe'] = np.log(bin_df['badattr']/bin_df['goodattr'])\n",
    "    bin_df['bin_iv'] = (bin_df['badattr']-bin_df['goodattr'])*bin_df['woe']\n",
    "    bin_df['IV'] = bin_df['bin_iv'].sum()\n",
    "    \n",
    "    return bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_corr_delete(df,col_list):\n",
    "    \"\"\"\n",
    "    相关性筛选,设定的阈值为0.65\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        col_list -- 需要筛选的特征集合,需要提前按IV值从大到小排序好 list\n",
    "    return:\n",
    "        select_corr_col -- 筛选后的特征集合 list\n",
    "    \"\"\"\n",
    "    corr_list=[]\n",
    "    corr_list.append(col_list[0])\n",
    "    delete_col = []\n",
    "    # 根据IV值的大小进行遍历\n",
    "    for col in col_list[1:]:\n",
    "        corr_list.append(col)\n",
    "        corr = df.loc[:,corr_list].corr()\n",
    "        corr_tup = [(x,y) for x,y in zip(corr[col].index,corr[col].values)]\n",
    "        corr_value = [y for x,y in corr_tup if x!=col]\n",
    "        # 若出现相关系数大于0.65，则将该特征剔除\n",
    "        if len([x for x in corr_value if abs(x)>=0.65])>0:\n",
    "            delete_col.append(col)\n",
    "    select_corr_col = [x for x in col_list if x not in delete_col]\n",
    "    return select_corr_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_delete(df,list_corr):\n",
    "    \"\"\"\n",
    "    多重共线性筛选\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        list_corr -- 相关性筛选后的特征集合，按IV值从大到小排序 list\n",
    "    return:\n",
    "        col_list -- 筛选后的特征集合 list\n",
    "    \"\"\"\n",
    "    col_list = list_corr.copy()\n",
    "    # 计算各个特征的方差膨胀因子\n",
    "    vif_matrix=np.matrix(df[col_list])\n",
    "    vifs_list=[variance_inflation_factor(vif_matrix,i) for i in range(vif_matrix.shape[1])]\n",
    "    # 筛选出系数>10的特征\n",
    "    vif_high = [x for x,y in zip(col_list,vifs_list) if y>10]\n",
    "    \n",
    "    # 根据IV从小到大的顺序进行遍历\n",
    "    if len(vif_high)>0:\n",
    "        for col in reversed(vif_high):\n",
    "            col_list.remove(col)\n",
    "            vif_matrix=np.matrix(df[col_list])\n",
    "            vifs=[variance_inflation_factor(vif_matrix,i) for i in range(vif_matrix.shape[1])]\n",
    "            # 当系数矩阵里没有>10的特征时，循环停止\n",
    "            if len([x for x in vifs if x>10])==0:\n",
    "                break\n",
    "    return col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pvalue_delete(x,y):\n",
    "    \"\"\"\n",
    "    显著性筛选，前向逐步回归\n",
    "    param:\n",
    "        x -- 特征数据集,woe转化后，且字段顺序按IV值从大到小排列 Dataframe\n",
    "        y -- 标签列 Series\n",
    "    return:\n",
    "        pvalues_col -- 筛选后的特征集合 list\n",
    "    \"\"\"\n",
    "    col_list = x.columns.tolist()\n",
    "    pvalues_col=[]\n",
    "    # 按IV值逐个引入模型\n",
    "    for col in col_list:\n",
    "        pvalues_col.append(col)\n",
    "        # 每引入一个特征就做一次显著性检验\n",
    "        x_const = sm.add_constant(x.loc[:,pvalues_col])\n",
    "        sm_lr = sm.Logit(y,x_const)\n",
    "        sm_lr = sm_lr.fit()\n",
    "        pvalue = sm_lr.pvalues[col]\n",
    "        # 当引入的特征P值>=0.05时，则剔除，原先满足显著性检验的则保留，不再剔除\n",
    "        if pvalue>=0.05:\n",
    "            pvalues_col.remove(col)\n",
    "    return pvalues_col\n",
    "\n",
    "\n",
    "def backward_pvalue_delete(x,y):\n",
    "    \"\"\"\n",
    "    显著性筛选，后向逐步回归\n",
    "    param:\n",
    "        x -- 特征数据集,woe转化后，且字段顺序按IV值从大到小排列 Dataframe\n",
    "        y -- 标签列 Series\n",
    "    return:\n",
    "        pvalues_col -- 筛选后的特征集合 list\n",
    "    \"\"\"\n",
    "    x_c = x.copy()\n",
    "    # 所有特征引入模型，做显著性检验\n",
    "    x_const = sm.add_constant(x_c)\n",
    "    sm_lr = sm.Logit(y,x_const).fit()\n",
    "    pvalue_tup = [(i,j) for i,j in zip(sm_lr.pvalues.index,sm_lr.pvalues.values)][1:]\n",
    "    delete_count = len([i for i,j in pvalue_tup if j>=0.05])\n",
    "    # 当有P值>=0.05的特征时，执行循环\n",
    "    while delete_count>0:\n",
    "        # 按IV值从小到大的顺序依次逐个剔除\n",
    "        remove_col = [i for i,j in pvalue_tup if j>=0.05][-1]\n",
    "        del x_c[remove_col]\n",
    "        # 每次剔除特征后都要重新做显著性检验，直到入模的特征P值都小于0.05\n",
    "        x2_const = sm.add_constant(x_c)\n",
    "        sm_lr2 = sm.Logit(y,x2_const).fit()\n",
    "        pvalue_tup2 = [(i,j) for i,j in zip(sm_lr2.pvalues.index,sm_lr2.pvalues.values)][1:]\n",
    "        delete_count = len([i for i,j in pvalue_tup2 if j>=0.05])\n",
    "        \n",
    "    pvalues_col = x_c.columns.tolist()\n",
    "    \n",
    "    return pvalues_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_delete_coef(x,y):\n",
    "    \"\"\"\n",
    "    系数一致筛选\n",
    "    param:\n",
    "        x -- 特征数据集,woe转化后，且字段顺序按IV值从大到小排列 Dataframe\n",
    "        y -- 标签列 Series\n",
    "    return:\n",
    "        coef_col -- 筛选后的特征集合 list\n",
    "    \"\"\"\n",
    "    col_list = list(x.columns)\n",
    "    coef_col = []\n",
    "    # 按IV值逐个引入模型，输出系数\n",
    "    for col in col_list:\n",
    "        coef_col.append(col)\n",
    "        x2 = x.loc[:,coef_col]\n",
    "        sk_lr = LogisticRegression(random_state=0).fit(x2,y)\n",
    "        coef_dict = {k:v for k,v in zip(coef_col,sk_lr.coef_[0])}\n",
    "        # 当引入特征的系数为负，则将其剔除\n",
    "        if coef_dict[col]<0:\n",
    "            coef_col.remove(col)\n",
    "\n",
    "    return coef_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_df(bin_df_list):\n",
    "    \"\"\"\n",
    "    得到特征woe映射集合表\n",
    "    param:\n",
    "        bin_df_list -- 每个特征的woe映射表 list\n",
    "    return:\n",
    "        map_merge_df -- 特征woe映射集合表 Dataframe\n",
    "    \"\"\"\n",
    "    map_df_list=[]\n",
    "    for dd in bin_df_list:\n",
    "        # 添加特征名列\n",
    "        map_df = dd.reset_index().assign(col=dd.index.name).rename(columns={dd.index.name:'bin'})\n",
    "        # 将特征名列移到第一列，便于查看\n",
    "        temp1 = map_df['col']\n",
    "        temp2 = map_df.iloc[:,:-1]\n",
    "        map_df2 = pd.concat([temp1,temp2],axis=1)\n",
    "        map_df_list.append(map_df2)\n",
    "        \n",
    "    map_merge_df = pd.concat(map_df_list,axis=0)\n",
    "    \n",
    "    return map_merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_mapping(df,map_df,var_map,target):\n",
    "    \"\"\"\n",
    "    特征映射\n",
    "    param:\n",
    "        df -- 原始数据集 Dataframe\n",
    "        map_df -- 特征映射集合表 Dataframe\n",
    "        var_map -- map_df里映射的字段名，如\"woe\",\"score\" string\n",
    "        target -- 标签字段名 string\n",
    "    return:\n",
    "        df2 -- 映射后的数据集 Dataframe\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    # 去掉标签字段，遍历特征\n",
    "    for col in df2.drop([target],axis=1).columns:\n",
    "        x = df2[col]\n",
    "        # 找到特征的映射表\n",
    "        bin_map = map_df[map_df.col==col]\n",
    "        # 新建一个映射array，填充0\n",
    "        bin_res = np.array([0]*x.shape[0],dtype=float)\n",
    "        for i in bin_map.index:\n",
    "            # 每个箱的最小值和最大值\n",
    "            lower = bin_map['min_bin'][i]\n",
    "            upper = bin_map['max_bin'][i]\n",
    "            # 对于类别型特征，每个箱的lower和upper时一样的\n",
    "            if lower == upper:\n",
    "                x1 = x[np.where(x == lower)[0]]\n",
    "            # 连续型特征，左开右闭\n",
    "            else:\n",
    "                x1 = x[np.where((x>lower)&(x<=upper))[0]]\n",
    "            mask = np.in1d(x,x1)\n",
    "            # 映射array里填充对应的映射值\n",
    "            bin_res[mask] = bin_map[var_map][i]\n",
    "        bin_res = pd.Series(bin_res,index=x.index)\n",
    "        bin_res.name = x.name\n",
    "        # 将原始值替换为映射值\n",
    "        df2[col] = bin_res\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_label,y_pred):\n",
    "    \"\"\"\n",
    "    绘制roc曲线\n",
    "    param:\n",
    "        y_label -- 真实的y值 list/array\n",
    "        y_pred -- 预测的y值 list/array\n",
    "    return:\n",
    "        roc曲线\n",
    "    \"\"\"\n",
    "    tpr,fpr,threshold = metrics.roc_curve(y_label,y_pred) \n",
    "    AUC = metrics.roc_auc_score(y_label,y_pred) \n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(tpr,fpr,color='blue',label='AUC=%.3f'%AUC) \n",
    "    ax.plot([0,1],[0,1],'r--')\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_title('ROC')\n",
    "    ax.legend(loc='best')\n",
    "    return plt.show(ax)\n",
    "\n",
    "def plot_model_ks(y_label,y_pred):\n",
    "    \"\"\"\n",
    "    绘制ks曲线\n",
    "    param:\n",
    "        y_label -- 真实的y值 list/array\n",
    "        y_pred -- 预测的y值 list/array\n",
    "    return:\n",
    "        ks曲线\n",
    "    \"\"\"\n",
    "    pred_list = list(y_pred) \n",
    "    label_list = list(y_label)\n",
    "    total_bad = sum(label_list)\n",
    "    total_good = len(label_list)-total_bad \n",
    "    items = sorted(zip(pred_list,label_list),key=lambda x:x[0]) \n",
    "    step = (max(pred_list)-min(pred_list))/200 \n",
    "    \n",
    "    pred_bin=[]\n",
    "    good_rate=[] \n",
    "    bad_rate=[] \n",
    "    ks_list = [] \n",
    "    for i in range(1,201): \n",
    "        idx = min(pred_list)+i*step \n",
    "        pred_bin.append(idx) \n",
    "        label_bin = [x[1] for x in items if x[0]<idx] \n",
    "        bad_num = sum(label_bin)\n",
    "        good_num = len(label_bin)-bad_num  \n",
    "        goodrate = good_num/total_good \n",
    "        badrate = bad_num/total_bad\n",
    "        ks = abs(goodrate-badrate) \n",
    "        good_rate.append(goodrate)\n",
    "        bad_rate.append(badrate)\n",
    "        ks_list.append(ks)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(pred_bin,good_rate,color='green',label='good_rate')\n",
    "    ax.plot(pred_bin,bad_rate,color='red',label='bad_rate')\n",
    "    ax.plot(pred_bin,ks_list,color='blue',label='good-bad')\n",
    "    ax.set_title('KS:{:.3f}'.format(max(ks_list)))\n",
    "    ax.legend(loc='best')\n",
    "    return plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_scale(score,odds,PDO,model):\n",
    "    \"\"\"\n",
    "    计算分数校准的A，B值，基础分\n",
    "    param:\n",
    "        odds：设定的坏好比 float\n",
    "        score: 在这个odds下的分数 int\n",
    "        PDO: 好坏翻倍比 int\n",
    "        model:模型\n",
    "    return:\n",
    "        A,B,base_score(基础分)\n",
    "    \"\"\"\n",
    "    B = 20/(np.log(odds)-np.log(2*odds))\n",
    "    A = score-B*np.log(odds)\n",
    "    base_score = A+B*model.intercept_[0]\n",
    "    return A,B,base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_map(woe_df,coe_dict,B):\n",
    "    \"\"\"\n",
    "    得到特征score的映射集合表\n",
    "    param:\n",
    "        woe_df -- woe映射集合表 Dataframe\n",
    "        coe_dict -- 系数对应的字典\n",
    "    return:\n",
    "        score_df -- score的映射集合表 Dataframe\n",
    "    \"\"\"\n",
    "    scores=[]\n",
    "    for cc in woe_df.col.unique():\n",
    "        woe_list = woe_df[woe_df.col==cc]['woe'].tolist()\n",
    "        coe = coe_dict[cc]\n",
    "        score = [round(coe*B*w,0) for w in woe_list]\n",
    "        scores.extend(score)\n",
    "    woe_df['score'] = scores\n",
    "    score_df = woe_df.copy()\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_hist(df,target,score_col,title,plt_size=None):\n",
    "    \"\"\"\n",
    "    绘制好坏用户得分分布图\n",
    "    param:\n",
    "        df -- 数据集 Dataframe\n",
    "        target -- 标签字段名 string\n",
    "        score_col -- 模型分的字段名 string\n",
    "        plt_size -- 绘图尺寸 tuple\n",
    "        title -- 图表标题 string\n",
    "    return:\n",
    "        好坏用户得分分布图\n",
    "    \"\"\"    \n",
    "    plt.figure(figsize=plt_size)\n",
    "    plt.title(title)\n",
    "    x1 = df[df[target]==1][score_col]\n",
    "    x2 = df[df[target]==0][score_col]\n",
    "    sns.kdeplot(x1,shade=True,label='bad',color='hotpink')\n",
    "    sns.kdeplot(x2,shade=True,label='good',color ='seagreen')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scorecard_model(train_data,test_data,target,nan_value=-999):\n",
    "    \"\"\"\n",
    "    评分卡建模\n",
    "    param:\n",
    "        train_data -- 训练数据集，预处理好的 Dataframe\n",
    "        test_data -- 测试数据集，预处理好的 Dataframe\n",
    "        target -- 标签字段名 string\n",
    "        nan_value -- 缺失的映射值 int 默认-999\n",
    "        odds -- 设定的坏好比 float 默认999/1\n",
    "        score -- 在这个odds下的分数 int 默认400\n",
    "        PDO -- 好坏翻倍比 int 默认20\n",
    "    return:\n",
    "        lr_model -- lr模型\n",
    "        score_map_df -- woe,score映射集合表 Dataframe\n",
    "        valid_score -- 验证集模型分表 Dataframe\n",
    "        test_score -- 测试集模型分表 Dataframe\n",
    " \n",
    "    # psi筛选，剔除psi大于0.25以上的特征\n",
    "    all_col = [x for x in train_data.columns if x!=target]\n",
    "    psi_tup = []\n",
    "    for col in all_col:\n",
    "        psi,psi_bin_df = cal_psi(train_data,test_data,col)\n",
    "        psi_tup.append((col,psi))\n",
    "    psi_delete = [x for x,y in psi_tup if y>=0.25]\n",
    "    train = train_data.drop(psi_delete,axis=1)\n",
    "    print('psi筛选特征完成')\n",
    "    print('-------------')\n",
    " \"\"\"\n",
    "    # 特征分箱,默认用的是决策树分箱\n",
    "    \n",
    "    train = train_data\n",
    "    train_col = [x for x in train.columns if x!=target]\n",
    "    bin_df_list=[]\n",
    "    cut_list=[]\n",
    "    for col in train_col:\n",
    "        try:\n",
    "            bin_df,cut = binning_var(train,col,target)\n",
    "            bin_df_list.append(bin_df)\n",
    "            cut_list.append(cut)\n",
    "        except:\n",
    "            pass\n",
    "    print('特征分箱完成')\n",
    "    print('-------------')\n",
    "    \n",
    "    # 剔除iv无限大的特征\n",
    "    bin_df_list = [x for x in bin_df_list if x.IV.iloc[0]!=float('inf')]\n",
    "    # 保存每个特征的分割点list\n",
    "    cut_dict={}\n",
    "    for dd,cc in zip(bin_df_list,cut_list):\n",
    "        col = dd.index.name\n",
    "        cut_dict[col] = cc\n",
    "    # 将IV从大到小进行排序\n",
    "    iv_col = [x.index.name for x in bin_df_list]\n",
    "    iv_value = [x.IV.iloc[0] for x in bin_df_list]\n",
    "    iv_sort = sorted(zip(iv_col,iv_value),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "    # iv筛选，筛选iv大于0.02的特征\n",
    "    iv_select_col = [x for x,y in iv_sort if y>=0.04]\n",
    "    print('iv筛选特征完成')\n",
    "    print('-------------')\n",
    "    # 特征分类\n",
    "    cate_col = []\n",
    "    num_col = []\n",
    "    for col in iv_select_col:\n",
    "        if train[col].dtype==np.dtype('object') or train[col].dtype==np.dtype('bool') or train[col].nunique()<=5:\n",
    "            cate_col.append(col)\n",
    "        else:\n",
    "            num_col.append(col)\n",
    "\n",
    "    #相关性筛选，相关系数阈值0.65\n",
    "    corr_select_col = forward_corr_delete(train,num_col)\n",
    "    print('相关性筛选完成')\n",
    "    print('-------------')\n",
    "\n",
    "    # 多重共线性筛选，系数阈值10\n",
    "    vif_select_col = vif_delete(train,corr_select_col)\n",
    "    print(vif_select_col)\n",
    "    print('多重共线性筛选完成')\n",
    "    print('-------------')\n",
    "    \n",
    "\n",
    "    # 自动调整单调分箱\n",
    "    trim_var_dict = {k:v for k,v in cut_dict.items() if k in vif_select_col}\n",
    "    trim_bin_list=[]\n",
    "    for col in trim_var_dict.keys():\n",
    "        bin_cut = trim_var_dict[col]\n",
    "        df_bin = [x for x in bin_df_list if x.index.name==col][0]\n",
    "        woe_lst = df_bin['woe'].tolist()\n",
    "        if not judge_decreasing(woe_lst) and not judge_increasing(woe_lst):\n",
    "            monot_cut = monot_trim(train, col, target, nan_value=nan_value, cut=bin_cut)\n",
    "            monot_bin_df = binning_trim(train, col, target, cut=monot_cut, right_border=True)\n",
    "            trim_bin_list.append(monot_bin_df)\n",
    "        else:\n",
    "            trim_bin_list.append(df_bin)\n",
    "    # 调整后的分箱再根据iv筛选一遍\n",
    "    select_num_df = []\n",
    "    for dd in trim_bin_list:\n",
    "        if dd.IV.iloc[0]>=0.02:\n",
    "            select_num_df.append(dd)\n",
    "    print('自动调整单调分箱完成')\n",
    "    print('-------------')\n",
    "    print(select_num_df)\n",
    "  \n",
    "    # 连续型特征的woe映射集合表\n",
    "    woe_map_num = get_map_df(select_num_df)\n",
    "    woe_map_num['bin'] = woe_map_num['bin'].map(lambda x:str(x))\n",
    "    woe_map_num['min_bin'] = woe_map_num['bin'].map(lambda x:x.split(',')[0][1:])\n",
    "    woe_map_num['max_bin'] = woe_map_num['bin'].map(lambda x:x.split(',')[1][:-1])\n",
    "    woe_map_num['min_bin'] = woe_map_num['min_bin'].map(lambda x:float(x))\n",
    "    woe_map_num['max_bin'] = woe_map_num['max_bin'].map(lambda x:float(x))\n",
    "    \n",
    "    if len(cate_col)>0:\n",
    "        bin_cate_list = [x for x in bin_df_list if x.index.name in cate_col]\n",
    "        # 剔除woe不单调的离散形特征\n",
    "        select_cate_df=[]\n",
    "        for i,dd in enumerate(bin_cate_list):\n",
    "            woe_lst = dd['woe'].tolist()\n",
    "            if judge_decreasing(woe_lst) or judge_increasing(woe_lst):\n",
    "                select_cate_df.append(dd)\n",
    "        # 离散型特征的woe映射集合表\n",
    "        if len(select_cate_df)>0:\n",
    "            woe_map_cate = get_map_df(select_cate_df)\n",
    "            woe_map_cate['min_bin'] = list(woe_map_cate['bin'])\n",
    "            woe_map_cate['max_bin'] = list(woe_map_cate['bin'])\n",
    "            woe_map_df = pd.concat([woe_map_cate,woe_map_num],axis=0).reset_index(drop=True)\n",
    "    else:\n",
    "        woe_map_df = woe_map_num.reset_index(drop=True)\n",
    "\n",
    "    # 显著性筛选，前向逐步回归\n",
    "    select_all_col = woe_map_df['col'].unique().tolist()\n",
    "    select_sort_col = [x for x,y in iv_sort if x in select_all_col]\n",
    "    \n",
    "    train2 = train.loc[:,select_sort_col+[target]].reset_index(drop=True)\n",
    "    # woe映射\n",
    "    train_woe = var_mapping(train2,woe_map_df,'woe',target)\n",
    "    X = train_woe.loc[:,select_sort_col]\n",
    "    y = train_woe[target]\n",
    "\n",
    "    pvalue_select_col = forward_pvalue_delete(X,y)\n",
    "    print('显著性筛选完成')\n",
    "    print('-------------')\n",
    "\n",
    "    # 剔除系数为负数的特征\n",
    "    X2 = X.loc[:,pvalue_select_col]\n",
    "    coef_select_col = forward_delete_coef(X2,y)\n",
    "\n",
    "    # LR建模\n",
    "    X3 = X2.loc[:,coef_select_col]\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(X3,y,test_size=0.2,random_state=0)\n",
    "    # 保存验证集的index\n",
    "    valid_index = x_valid.index.tolist()\n",
    "    \n",
    "    lr_model = LogisticRegression(C=1.0).fit(x_train,y_train)\n",
    "    print('建模完成')\n",
    "    print('-------------')\n",
    "    \n",
    "    # 绘制验证集的auc，ks\n",
    "    valid_pre = lr_model.predict_proba(x_valid)[:,1]\n",
    "    print('验证集的AUC，KS:')\n",
    "    plot_roc(y_valid,valid_pre)\n",
    "    plot_model_ks(y_valid,valid_pre)\n",
    "    \n",
    "    woe_map_df2 = woe_map_df[woe_map_df.col.isin(coef_select_col)].reset_index(drop=True)\n",
    "    # 绘制测试集的auc，ks\n",
    "    test = test_data.loc[:,coef_select_col+[target]].reset_index(drop=True)\n",
    "    test_woe = var_mapping(test,woe_map_df2,'woe',target)\n",
    "    x_test = test_woe.drop([target],axis=1)\n",
    "    y_test = test_woe[target]\n",
    "    test_pre = lr_model.predict_proba(x_test)[:,1]\n",
    "    print('测试集的AUC，KS:')\n",
    "    plot_roc(y_test,test_pre)\n",
    "    plot_model_ks(y_test,test_pre)\n",
    "      \n",
    "    return lr_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('C:/Users/Wu Ran/Desktop/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('C:/Users/Wu Ran/Desktop/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征分箱完成\n",
      "-------------\n",
      "iv筛选特征完成\n",
      "-------------\n",
      "相关性筛选完成\n",
      "-------------\n",
      "['ali_score', 'be_query_cnt_9', 'onlinebuying', 'weight_be_applied', 'weight_to_applied', 'time_spent_be_applied', 'zc_credit_score', '12_diff_month_sum_repay_remind_average_level', 'worktimeshopping', '54_natural_week_sum_overdue_repay_count', '3m_apply_count_score', 'weight_to_black', 'times_by_other_org', '24_diff_month_sum_loan_offer_count', 'm24_verif_sum', 'riskscore', 'contacts_class2_blacklist_cnt', 'time_spent_be_black', '24_diff_month_sum_loan_offer_average_level', '48_natural_week_sum_apply_request_average_level', '24_natural_week_sum_repay_fail_average_level', '54_natural_week_sum_overdue_repay_average_level', '6_diff_month_sum_repay_fail_count', 'call_cnt_be_black', '7d_apply_count_score']\n",
      "多重共线性筛选完成\n",
      "-------------\n",
      "自动调整单调分箱完成\n",
      "-------------\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-2df9d8c5d337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_scorecard_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-c7cfbdbd091a>\u001b[0m in \u001b[0;36mget_scorecard_model\u001b[1;34m(train_data, test_data, target, nan_value)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# 连续型特征的woe映射集合表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mwoe_map_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_map_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect_num_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mwoe_map_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwoe_map_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mwoe_map_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_bin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwoe_map_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-7026959b2ca9>\u001b[0m in \u001b[0;36mget_map_df\u001b[1;34m(bin_df_list)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmap_df_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_df2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmap_merge_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_df_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmap_merge_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "get_scorecard_model(train_data,test_data,'y',nan_value=-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scorecard_model(train_data,test_data,target,nan_value=-999,score=600,odds=9/1,pdo=2):\n",
    "    \"\"\"\n",
    "    评分卡建模\n",
    "    param:\n",
    "        train_data -- 训练数据集，预处理好的 Dataframe\n",
    "        test_data -- 测试数据集，预处理好的 Dataframe\n",
    "        target -- 标签字段名 string\n",
    "        nan_value -- 缺失的映射值 int 默认-999\n",
    "        odds -- 设定的坏好比 float 默认999/1\n",
    "        score -- 在这个odds下的分数 int 默认400\n",
    "        PDO -- 好坏翻倍比 int 默认20\n",
    "    return:\n",
    "        lr_model -- lr模型\n",
    "        score_map_df -- woe,score映射集合表 Dataframe\n",
    "        valid_score -- 验证集模型分表 Dataframe\n",
    "        test_score -- 测试集模型分表 Dataframe\n",
    "    \"\"\"\n",
    "    # psi筛选，剔除psi大于0.25以上的特征\n",
    "    all_col = [x for x in train_data.columns if x!=target]\n",
    "    psi_tup = []\n",
    "    for col in all_col:\n",
    "        psi,psi_bin_df = cal_psi(train_data,test_data,col)\n",
    "        psi_tup.append((col,psi))\n",
    "    psi_delete = [x for x,y in psi_tup if y>=0.25]\n",
    "    train = train_data.drop(psi_delete,axis=1)\n",
    "    print('psi筛选特征完成')\n",
    "    print('-------------')\n",
    "\n",
    "    # 特征分箱,默认用的是决策树分箱\n",
    "    train_col = [x for x in train.columns if x!=target]\n",
    "    bin_df_list=[]\n",
    "    cut_list=[]\n",
    "    for col in train_col:\n",
    "        try:\n",
    "            bin_df,cut = binning_var(train,col,target)\n",
    "            bin_df_list.append(bin_df)\n",
    "            cut_list.append(cut)\n",
    "        except:\n",
    "            pass\n",
    "    print('特征分箱完成')\n",
    "    print('-------------')\n",
    "    \n",
    "    # 剔除iv无限大的特征\n",
    "    bin_df_list = [x for x in bin_df_list if x.IV.iloc[0]!=float('inf')]\n",
    "    # 保存每个特征的分割点list\n",
    "    cut_dict={}\n",
    "    for dd,cc in zip(bin_df_list,cut_list):\n",
    "        col = dd.index.name\n",
    "        cut_dict[col] = cc\n",
    "    # 将IV从大到小进行排序\n",
    "    iv_col = [x.index.name for x in bin_df_list]\n",
    "    iv_value = [x.IV.iloc[0] for x in bin_df_list]\n",
    "    iv_sort = sorted(zip(iv_col,iv_value),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "    # iv筛选，筛选iv大于0.02的特征\n",
    "    iv_select_col = [x for x,y in iv_sort if y>=0.02]\n",
    "    print('iv筛选特征完成')\n",
    "    print('-------------')\n",
    "    # 特征分类\n",
    "    cate_col = []\n",
    "    num_col = []\n",
    "    for col in iv_select_col:\n",
    "        if train[col].dtype==np.dtype('object') or train[col].dtype==np.dtype('bool') or train[col].nunique()<=5:\n",
    "            cate_col.append(col)\n",
    "        else:\n",
    "            num_col.append(col)\n",
    "\n",
    "    #相关性筛选，相关系数阈值0.65\n",
    "    corr_select_col = forward_corr_delete(train,num_col)\n",
    "    print('相关性筛选完成')\n",
    "    print('-------------')\n",
    "\n",
    "    # 多重共线性筛选，系数阈值10\n",
    "    vif_select_col = vif_delete(train,corr_select_col)\n",
    "    print('多重共线性筛选完成')\n",
    "    print('-------------')\n",
    "    #print(vif_select_col)\n",
    "\n",
    "    # 自动调整单调分箱\n",
    "    trim_var_dict = {k:v for k,v in cut_dict.items() if k in vif_select_col}\n",
    "    trim_bin_list=[]\n",
    "    for col in trim_var_dict.keys():\n",
    "        bin_cut = trim_var_dict[col]\n",
    "        df_bin = [x for x in bin_df_list if x.index.name==col][0]\n",
    "        woe_lst = df_bin['woe'].tolist()\n",
    "        if not judge_decreasing(woe_lst) and not judge_increasing(woe_lst):\n",
    "            monot_cut = monot_trim(train, col, target, nan_value=nan_value, cut=bin_cut)\n",
    "            monot_bin_df = binning_trim(train, col, target, cut=monot_cut, right_border=True)\n",
    "            trim_bin_list.append(monot_bin_df)\n",
    "        else:\n",
    "            trim_bin_list.append(df_bin)\n",
    "    # 调整后的分箱再根据iv筛选一遍\n",
    "    select_num_df = []\n",
    "    for dd in trim_bin_list:\n",
    "        if dd.IV.iloc[0]>=0.02:\n",
    "            select_num_df.append(dd)\n",
    "    print('自动调整单调分箱完成')\n",
    "    print('-------------')\n",
    "    print(select_num_df)\n",
    "    \n",
    "    # 连续型特征的woe映射集合表\n",
    "    woe_map_num = get_map_df(select_num_df)\n",
    "    woe_map_num['bin'] = woe_map_num['bin'].map(lambda x:str(x))\n",
    "    woe_map_num['min_bin'] = woe_map_num['bin'].map(lambda x:x.split(',')[0][1:])\n",
    "    woe_map_num['max_bin'] = woe_map_num['bin'].map(lambda x:x.split(',')[1][:-1])\n",
    "    woe_map_num['min_bin'] = woe_map_num['min_bin'].map(lambda x:float(x))\n",
    "    woe_map_num['max_bin'] = woe_map_num['max_bin'].map(lambda x:float(x))\n",
    "    \n",
    "    if len(cate_col)>0:\n",
    "        bin_cate_list = [x for x in bin_df_list if x.index.name in cate_col]\n",
    "        # 剔除woe不单调的离散形特征\n",
    "        select_cate_df=[]\n",
    "        for i,dd in enumerate(bin_cate_list):\n",
    "            woe_lst = dd['woe'].tolist()\n",
    "            if judge_decreasing(woe_lst) or judge_increasing(woe_lst):\n",
    "                select_cate_df.append(dd)\n",
    "        # 离散型特征的woe映射集合表\n",
    "        if len(select_cate_df)>0:\n",
    "            woe_map_cate = get_map_df(select_cate_df)\n",
    "            woe_map_cate['min_bin'] = list(woe_map_cate['bin'])\n",
    "            woe_map_cate['max_bin'] = list(woe_map_cate['bin'])\n",
    "            woe_map_df = pd.concat([woe_map_cate,woe_map_num],axis=0).reset_index(drop=True)\n",
    "    else:\n",
    "        woe_map_df = woe_map_num.reset_index(drop=True)\n",
    "\n",
    "    # 显著性筛选，前向逐步回归\n",
    "    select_all_col = woe_map_df['col'].unique().tolist()\n",
    "    print(select_all_col)\n",
    "    select_sort_col = [x for x,y in iv_sort if x in select_all_col]\n",
    "    \n",
    "    train2 = train.loc[:,select_sort_col+[target]].reset_index(drop=True)\n",
    "    # woe映射\n",
    "    train_woe = var_mapping(train2,woe_map_df,'woe',target)\n",
    "    X = train_woe.loc[:,select_sort_col]\n",
    "    y = train_woe[target]\n",
    "\n",
    "    pvalue_select_col = forward_pvalue_delete(X,y)\n",
    "    print('显著性筛选完成')\n",
    "    print('-------------')\n",
    "\n",
    "    # 剔除系数为负数的特征\n",
    "    X2 = X.loc[:,pvalue_select_col]\n",
    "    coef_select_col = forward_delete_coef(X2,y)\n",
    "\n",
    "    # LR建模\n",
    "    X3 = X2.loc[:,coef_select_col]\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(X3,y,test_size=0.2,random_state=0)\n",
    "    # 保存验证集的index\n",
    "    valid_index = x_valid.index.tolist()\n",
    "    \n",
    "    lr_model = LogisticRegression(C=1.0).fit(x_train,y_train)\n",
    "    print('建模完成')\n",
    "    print('-------------')\n",
    "    \n",
    "    # 绘制验证集的auc，ks\n",
    "    valid_pre = lr_model.predict_proba(x_valid)[:,1]\n",
    "    print('验证集的AUC，KS:')\n",
    "    plot_roc(y_valid,valid_pre)\n",
    "    plot_model_ks(y_valid,valid_pre)\n",
    "    \n",
    "    woe_map_df2 = woe_map_df[woe_map_df.col.isin(coef_select_col)].reset_index(drop=True)\n",
    "    # 绘制测试集的auc，ks\n",
    "    test = test_data.loc[:,coef_select_col+[target]].reset_index(drop=True)\n",
    "    test_woe = var_mapping(test,woe_map_df2,'woe',target)\n",
    "    x_test = test_woe.drop([target],axis=1)\n",
    "    y_test = test_woe[target]\n",
    "    test_pre = lr_model.predict_proba(x_test)[:,1]\n",
    "    print('测试集的AUC，KS:')\n",
    "    plot_roc(y_test,test_pre)\n",
    "    plot_model_ks(y_test,test_pre)\n",
    "      \n",
    "    # 评分转换\n",
    "    A,B,base_score = cal_scale(score,odds,pdo,lr_model)\n",
    "    score_map_df  = get_score_map(woe_map_df2,lr_model,B)\n",
    "    # 分数映射\n",
    "    valid_data = train2.iloc[valid_index,:].loc[:,coef_select_col+[target]].reset_index(drop=True)\n",
    "    valid_score = var_mapping(valid_data,score_map_df,'score',target)\n",
    "    valid_score['final_score'] = base_score\n",
    "    for col in coef_select_col:\n",
    "        valid_score['final_score']+=valid_score[col]\n",
    "    valid_score['final_score'] = valid_score['final_score'].map(lambda x:int(x))\n",
    "    \n",
    "    test_score = var_mapping(test,score_map_df,'score',target)\n",
    "    test_score['final_score'] = base_score\n",
    "    for col in coef_select_col:\n",
    "        test_score['final_score']+=test_score[col]\n",
    "    test_score['final_score'] = test_score['final_score'].map(lambda x:int(x))\n",
    "    print('评分转换完成')\n",
    "    print('-------------')\n",
    "    # 验证集的评分分布\n",
    "    plot_score_hist(valid_score, target, 'final_score','vaild_score',plt_size=(6,4))\n",
    "    # 测试集的评分分布\n",
    "    plot_score_hist(test_score, target, 'final_score','test_score',plt_size=(6,4))\n",
    "\n",
    "    return lr_model,score_map_df,valid_score,test_score\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.read_csv('C:/Users/Wu Ran/Desktop/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= pd.read_csv('C:/Users/Wu Ran/Desktop/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psi筛选特征完成\n",
      "-------------\n",
      "特征分箱完成\n",
      "-------------\n",
      "iv筛选特征完成\n",
      "-------------\n",
      "相关性筛选完成\n",
      "-------------\n",
      "多重共线性筛选完成\n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5fd82e2b7a34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_scorecard_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0modds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpdo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-d1ac286ecb52>\u001b[0m in \u001b[0;36mget_scorecard_model\u001b[1;34m(train_data, test_data, target, nan_value, score, odds, pdo)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mwoe_lst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_bin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'woe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjudge_decreasing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwoe_lst\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjudge_increasing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwoe_lst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mmonot_cut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonot_trim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnan_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbin_cut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mmonot_bin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinning_trim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmonot_cut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_border\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtrim_bin_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonot_bin_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2af7284f95a0>\u001b[0m in \u001b[0;36mmonot_trim\u001b[1;34m(df, col, target, nan_value, cut)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mindex_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjudge_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mnew_cut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mwoe_lst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcal_woe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_cut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_cut\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-90c652e20134>\u001b[0m in \u001b[0;36mcal_woe\u001b[1;34m(df, col, target, nan_value, cut)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mgood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mbucket\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36mcut\u001b[1;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates)\u001b[0m\n\u001b[0;32m    232\u001b[0m                               \u001b[0minclude_lowest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude_lowest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                               \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                               duplicates=duplicates)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     return _postprocess_for_cut(fac, bins, retbins, x_is_series,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[1;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates)\u001b[0m\n\u001b[0;32m    348\u001b[0m             labels = _format_labels(bins, precision, right=right,\n\u001b[0;32m    349\u001b[0m                                     \u001b[0minclude_lowest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude_lowest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                                     dtype=dtype)\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36m_format_labels\u001b[1;34m(bins, precision, right, include_lowest, dtype)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[0madjust\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mTimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1ns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_round_frac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0madjust\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36m_infer_precision\u001b[1;34m(base_precision, bins)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_precision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[0mlevels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_round_frac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbase_precision\u001b[0m  \u001b[1;31m# default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \"\"\"\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_arraylike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36m_ensure_arraylike\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \"\"\"\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_array_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[0minferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mixed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'unicode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.infer_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_scorecard_model(train_data,test_data,'y',nan_value=-999,score=400,odds=999/1,pdo=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
